# ğŸ¤– AI vs Human â€” AIâ€‘Generated Text Detector

A binary text classifier that detects whether a text is **human-written** or **generated by an LLM**.

This repository includes:

* ğŸš€ A trainable PyTorch model based on a lightweight Transformer encoder
* ğŸ“Š A data aggregation pipeline (Kaggle + Hugging Face)
* ğŸŒ A minimal FastAPI web app with an HTML UI
* ğŸ³ Docker setup for containerized runs

---

## ğŸ“‚ Project structure

```
ml/               # model and training code
  data_collection.py  # aggregate external datasets into a single CSV
  dataset.py          # PyTorch Dataset + collate_fn
  interface.py        # ModelWrapper for inference
  model.py            # architecture (Embedding + PosEnc + Encoders + head)
  tokenizer.py        # wrapper over HF AutoTokenizer (t5-small)
  training.py         # training script + TensorBoard logging

web_app/          # FastAPI application
  main.py             # endpoints: /, /predict, /health, /model-info
  templates/index.html# simple UI page
  static/style.css    # styles

models/           # model checkpoint (model.pth)
data/             # aggregated dataset (data.csv)

scripts/
  up.sh           # start server (uvicorn)

Dockerfile        # containerization
requirements.txt  # dependencies
```

---

## âš¡ Quick start (local)

1ï¸âƒ£ Install dependencies:

```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
```

2ï¸âƒ£ Start the web app:

```bash
bash scripts/up.sh
```

Open in your browser: [http://localhost:8000](http://localhost:8000) ğŸŒ

3ï¸âƒ£ Health check:

```bash
curl http://localhost:8000/health
```

---

## ğŸ³ Run with Docker

```bash
docker build -t ai-vs-human:latest .
docker run --rm -p 8000:8000 ai-vs-human:latest
```

Then visit [http://localhost:8000](http://localhost:8000)

---

## ğŸ—‚ï¸ Data preparation

Aggregate datasets into a single CSV:

```bash
python ml/data_collection.py
```

The dataset will be saved as `data/data.csv` with columns:

* `text` â€” raw text
* `generated` â€” 1 for AI, 0 for human

> âš ï¸ Note: Kaggle downloads require proper `kagglehub` credentials.

---

## ğŸ“ Training

Training parameters are in `ml/training.py` (batch size, epochs, device, etc.).
By default, GPU is used if available (`device='cuda'`).

Run training:

```bash
python ml/training.py
```

Metrics are logged to TensorBoard (`runs/â€¦`), and the checkpoint is saved to `models/model.pth`.

---

## ğŸ—ï¸ Model architecture (brief)

* Embeddings: `nn.Embedding(num_embeddings, 300)`
* Positional encoding: sinusoidal, computed on the fly
* Encoder stack: multiple `Encoder` blocks (LayerNorm + MHA + FFN + residual)
* Classifier: `[CLS]` pooling â†’ `Linear(300â†’200)` + ReLU â†’ `Linear(200â†’1)`
* Loss: `BCEWithLogitsLoss`
* Optimizer: `Adam`
* AMP: `torch.amp` for GPU acceleration âš¡

---

## âœ‚ï¸ Tokenization

Wrapper around `transformers.AutoTokenizer` (t5-small).

* Prefix `"<extra_id_0>"` added for single inputs
* Returns batch-first tensor with padding for multiple strings

---

## ğŸŒ Web API

FastAPI service (`web_app/main.py`)

Endpoints:

* `GET /` â€” HTML page with text input form
* `POST /predict` â€” inference

  * Body: `{ "text": string | string[] }`
  * Response: `{ "input": ..., "prediction": number[] }` or `{ "prediction": [number] }`
* `GET /health` â€” `{"status": "ok"}`
* `GET /model-info` â€” version, date, F1, etc.

Example request:

```bash
curl -X POST http://localhost:8000/predict \
  -H 'Content-Type: application/json' \
  -d '{"text": "This is a sample text"}'
```

Example response:

```json
{ "input": "This is a sample text", "prediction": [0.73] }
```

> ğŸ’¡ The HTML page expects a scalar; use `prediction[0]` for single strings.

---

## ğŸ“ˆ TensorBoard

After training:

```bash
tensorboard --logdir runs
```

Open [http://localhost:6006](http://localhost:6006)

---

## ğŸ“¦ Requirements

Main ones:

* `torch`;
* `transformers`; 
* `fastapi`; 
* `uvicorn`; 
* `kagglehub`.

---

## âš™ï¸ Known notes/settings

* `ml/interface.py`: device auto-selected (`cuda` if available)
* `ml/training.py`: default `epoch_count=1` (adjust for your setup)
* Max sequence length truncated to 512 tokens

---

## ğŸ“„ License

See LICENSE at the repository root
