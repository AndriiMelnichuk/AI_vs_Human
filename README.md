# 🤖 AI vs Human — AI‑Generated Text Detector

A binary text classifier that detects whether a text is **human-written** or **generated by an LLM**.

This repository includes:

* 🚀 A trainable PyTorch model based on a lightweight Transformer encoder
* 📊 A data aggregation pipeline (Kaggle + Hugging Face)
* 🌐 A minimal FastAPI web app with an HTML UI
* 🐳 Docker setup for containerized runs

---

## 📂 Project structure

```
ml/               # model and training code
  data_collection.py  # aggregate external datasets into a single CSV
  dataset.py          # PyTorch Dataset + collate_fn
  interface.py        # ModelWrapper for inference
  model.py            # architecture (Embedding + PosEnc + Encoders + head)
  tokenizer.py        # wrapper over HF AutoTokenizer (t5-small)
  training.py         # training script + TensorBoard logging

web_app/          # FastAPI application
  main.py             # endpoints: /, /predict, /health, /model-info
  templates/index.html# simple UI page
  static/style.css    # styles

models/           # model checkpoint (model.pth)
data/             # aggregated dataset (data.csv)

scripts/
  up.sh           # start server (uvicorn)

Dockerfile        # containerization
requirements.txt  # dependencies
```

---

## ⚡ Quick start (local)

1️⃣ Install dependencies:

```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
```

2️⃣ Start the web app:

```bash
bash scripts/up.sh
```

Open in your browser: [http://localhost:8000](http://localhost:8000) 🌐

3️⃣ Health check:

```bash
curl http://localhost:8000/health
```

---

## 🐳 Run with Docker

```bash
docker build -t ai-vs-human:latest .
docker run --rm -p 8000:8000 ai-vs-human:latest
```

Then visit [http://localhost:8000](http://localhost:8000)

---

## 🗂️ Data preparation

Aggregate datasets into a single CSV:

```bash
python ml/data_collection.py
```

The dataset will be saved as `data/data.csv` with columns:

* `text` — raw text
* `generated` — 1 for AI, 0 for human

> ⚠️ Note: Kaggle downloads require proper `kagglehub` credentials.

---

## 🎓 Training

Training parameters are in `ml/training.py` (batch size, epochs, device, etc.).
By default, GPU is used if available (`device='cuda'`).

Run training:

```bash
python ml/training.py
```

Metrics are logged to TensorBoard (`runs/…`), and the checkpoint is saved to `models/model.pth`.

---

## 🏗️ Model architecture (brief)

* Embeddings: `nn.Embedding(num_embeddings, 300)`
* Positional encoding: sinusoidal, computed on the fly
* Encoder stack: multiple `Encoder` blocks (LayerNorm + MHA + FFN + residual)
* Classifier: `[CLS]` pooling → `Linear(300→200)` + ReLU → `Linear(200→1)`
* Loss: `BCEWithLogitsLoss`
* Optimizer: `Adam`
* AMP: `torch.amp` for GPU acceleration ⚡

---

## ✂️ Tokenization

Wrapper around `transformers.AutoTokenizer` (t5-small).

* Prefix `"<extra_id_0>"` added for single inputs
* Returns batch-first tensor with padding for multiple strings

---

## 🌐 Web API

FastAPI service (`web_app/main.py`)

Endpoints:

* `GET /` — HTML page with text input form
* `POST /predict` — inference

  * Body: `{ "text": string | string[] }`
  * Response: `{ "input": ..., "prediction": number[] }` or `{ "prediction": [number] }`
* `GET /health` — `{"status": "ok"}`
* `GET /model-info` — version, date, F1, etc.

Example request:

```bash
curl -X POST http://localhost:8000/predict \
  -H 'Content-Type: application/json' \
  -d '{"text": "This is a sample text"}'
```

Example response:

```json
{ "input": "This is a sample text", "prediction": [0.73] }
```

> 💡 The HTML page expects a scalar; use `prediction[0]` for single strings.

---

## 📈 TensorBoard

After training:

```bash
tensorboard --logdir runs
```

Open [http://localhost:6006](http://localhost:6006)

---

## 📦 Requirements

Main ones:

* `torch`;
* `transformers`; 
* `fastapi`; 
* `uvicorn`; 
* `kagglehub`.

---

## ⚙️ Known notes/settings

* `ml/interface.py`: device auto-selected (`cuda` if available)
* `ml/training.py`: default `epoch_count=1` (adjust for your setup)
* Max sequence length truncated to 512 tokens

---

## 📄 License

See LICENSE at the repository root
